{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b2d6723-1263-4527-b426-5f633abff40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT HERE ALL FOLDERS TO BE PREPROCESSED:\n",
    "# all recordings and probes (except for LFP) within the following paths will be pre-processed.\n",
    "# specify the path to a general folder containing multiple experiments, or be more specific if you need to exclude something:\n",
    "\n",
    "base_npxpaths = [r\"D:\\M21\"]\n",
    "                # r\"G:\\Paola Storage do not delete\\RL_E1\\E1_M18\",\n",
    "                # r\"G:\\Paola Storage do not delete\\RL_E1\\E1_M16\",\n",
    "run_barcodeSync = False\n",
    "run_preprocessing = True # run preprocessing and spikesorting\n",
    "callKSfromSI = False  # this remains insanely slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95e0c296-d890-4db7-ae68-b27e6cd1e780",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import scipy.signal\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "import spikeinterface.extractors as se\n",
    "import spikeinterface.full as si\n",
    "import spikeinterface.preprocessing as st\n",
    "import spikeinterface.sorters as ss\n",
    "\n",
    "from probeinterface import ProbeGroup, write_prb\n",
    "from probeinterface.plotting import plot_probe, plot_probe_group\n",
    "\n",
    "from kilosort import io, run_kilosort\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "#%matplotlib  widget\n",
    "\n",
    "n_cpus = os.cpu_count()\n",
    "n_jobs = n_cpus - 4\n",
    "print(n_jobs)\n",
    "job_kwargs = dict(n_jobs=n_jobs, chunk_duration=\"1s\", progress_bar=True)\n",
    "\n",
    "dtype = np.int16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3bc76e1-9053-4d09-99d7-a2c6483a6f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEXT_FEATURES (tentative):\n",
    "# add programmatic call to KS (not via SI - or perhaps not without saving preprocessed data to disk.. otherwise takes impossibly long)\n",
    "\n",
    "# # add an option to either save data.bin and ks output to a different drive/location if a faster drive is available, and then transfer it all to the original destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc938dda-7b30-44eb-b099-d1c206554a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will be processing all probes from the following recordings:\n",
      "D:\\M21\\2024-04-21_16-51-50\\Record Node 103\\experiment1\\recording1\n",
      "ProbeA-AP\n",
      "ProbeB-AP\n",
      "[0, 2]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "found_paths = [] #List to store all found paths\n",
    "target_file = 'structure.oebin'\n",
    "\n",
    "def search_files(base_path, target_file):\n",
    "    for dirpath, dirnames, filenames in os.walk(base_path):\n",
    "        if target_file in filenames:\n",
    "            found_paths.append(dirpath)\n",
    "\n",
    "for path in base_npxpaths:\n",
    "    search_files(path, target_file)\n",
    "\n",
    "if found_paths:\n",
    "    print(\"Will be processing all probes from the following recordings:\")\n",
    "    for path in found_paths:\n",
    "        print(path)\n",
    "        full_path = os.path.join(path, target_file)\n",
    "        with open(full_path, 'r') as file:\n",
    "            file_contents = file.read()\n",
    "            data = json.loads(file_contents)\n",
    "            continuous_data = data['continuous']\n",
    "            indices_notLFP = [index for index, folder in enumerate(continuous_data)\n",
    "                                   if \"LFP\" not in folder['folder_name'] and \"NI-DAQ\" not in folder['folder_name']]\n",
    "            for index in indices_notLFP:\n",
    "                stream_name = continuous_data[index]['stream_name']\n",
    "                print(stream_name)\n",
    "else:\n",
    "    print(\"File 'structure.oebin' not found in any of the specified directories\")\n",
    "\n",
    "    \n",
    "# log relevant preprocessing metadata\n",
    "now = datetime.now()\n",
    "formatted_time = now.strftime('%Y%m%d_%H%M%S')\n",
    "logfilename = f\"log_preprocessing_{formatted_time}.json\" \n",
    "# initialize a dictionary that will be updated dynamically\n",
    "log_metadata = {\n",
    "    \"recording\": []\n",
    "}\n",
    "print(indices_notLFP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9de0d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find json file with recording structure\n",
    "from pprint import pprint\n",
    "base_path = base_npxpaths[0]\n",
    "\n",
    "def find_recinfo_file(base_path, target_file='structure.oebin'):\n",
    "    file_path = list(Path(base_path).rglob(target_file))\n",
    "    assert len(file_path) == 1, f\"Found {len(file_path)} files with name {target_file} in {base_path}. Please specify a single file.\"\n",
    "    return file_path[0]\n",
    "\n",
    "def read_continuous_data_info(recinfo_file):\n",
    "    with open(recinfo_file, 'r') as f:\n",
    "        file_contents = f.read()\n",
    "        rec_info = json.loads(file_contents)\n",
    "    # pprint(rec_info)\n",
    "    return rec_info[\"continuous\"]\n",
    "\n",
    "def get_channel_names(continuous_data_info):\n",
    "    string_to_exclude = ['LFP', 'NI-DAQ']\n",
    "    # recinfo = load_recinfo(base_path)\n",
    "    all_folders = []\n",
    "    ap_folders = []\n",
    "    for folder in continuous_data_info:\n",
    "        stream_name = folder[\"recorded_processor\"] + \" \" + str(folder[\"recorded_processor_id\"]) + \"#\" + folder['folder_name'][:-1]\n",
    "        all_folders.append(stream_name)\n",
    "\n",
    "        if all([string not in folder['folder_name'] for string in string_to_exclude]):\n",
    "            ap_folders.append(stream_name)\n",
    "    \n",
    "    return all_folders, ap_folders\n",
    "\n",
    "recinfo_file = find_recinfo_file(base_path)\n",
    "continuous_data_info = read_continuous_data_info(recinfo_file)\n",
    "all_stream_names, ap_stream_names = get_channel_names(continuous_data_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3722e981-c5b3-45e2-9989-c32962c01b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\M21\\2024-04-21_16-51-50\\Record Node 103\\experiment1\\recording1\\SynchData\n"
     ]
    }
   ],
   "source": [
    "# if sync data were collected through openephys, you can proceed directly (if you read this, it might need debugging)\n",
    "\n",
    "# if sync data were collected through intan (legacy), pause here and extract the barcode files first (done in matlab)\n",
    "# then move the SynchData folder to the corresponding paths shown above\n",
    "for path in found_paths:\n",
    "    sync_folder = Path(os.path.join(path, 'SynchData')) # please do not change the spelling\n",
    "    if not sync_folder.is_dir():\n",
    "        os.mkdir(sync_folder)\n",
    "    print(sync_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a8c15d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new dir:  D:\\M21\\2024-04-21_16-51-50\\Record Node 103\\experiment1\\recording1\\SpikeData_Record Node 103#Neuropix-PXI-100.ProbeB-AP\n",
      "Stream name:  Record Node 103#Neuropix-PXI-100.ProbeB-AP\n",
      "OpenEphysBinaryRecordingExtractor: 384 channels - 30.0kHz - 1 segments - 194,631,564 samples \n",
      "                                   6,487.72s (1.80 hours) - int16 dtype - 139.21 GiB\n",
      "Estimated LSB value: 12\n",
      "ChannelsAggregationRecording: 383 channels - 30.0kHz - 1 segments - 194,631,564 samples \n",
      "                              6,487.72s (1.80 hours) - int16 dtype - 138.85 GiB\n",
      "========================================\n",
      "Loading recording with SpikeInterface...\n",
      "number of samples: 194631564\n",
      "number of channels: 383\n",
      "numbef of segments: 1\n",
      "sampling rate: 30000.0\n",
      "dtype: int16\n"
     ]
    }
   ],
   "source": [
    "for base_path in base_npxpaths:\n",
    "    recinfo_file = find_recinfo_file(base_path)\n",
    "    continuous_data_info = read_continuous_data_info(recinfo_file)\n",
    "    all_stream_names, ap_stream_names = get_channel_names(continuous_data_info)\n",
    "\n",
    "    # extra step to correct for bad timestamps from failed online synchronization here:\n",
    "    sync_folder = Path(os.path.join(path, 'SynchData')) # please do not change the spelling\n",
    "    \n",
    "    if run_barcodeSync:\n",
    "        if not sync_folder.is_dir():\n",
    "            os.mkdir(sync_folder)\n",
    "        for continuous_stream_info in continuous_data_info:\n",
    "            foldername = continuous_stream_info['folder_name']\n",
    "            stream_name = continuous_stream_info['stream_name']\n",
    "            sample_rate = continuous_stream_info['sample_rate']\n",
    "            workdir = os.path.join(path, 'continuous', foldername)\n",
    "            sn = np.load(os.path.join(workdir, 'sample_numbers.npy'))\n",
    "            ts = sn.astype(np.float64)\n",
    "            ts = ts/sample_rate\n",
    "            ts = ts.reshape(-1,1) #make it consistent with what is required downstream, and with what is saved by matlab\n",
    "            print(ts.shape)\n",
    "            np.save(os.path.join(sync_folder, \"timestamps_{}.npy\".format(stream_name)), ts)\n",
    "            # also retrieve all the barcode files and copy them to the same sync folder\n",
    "            workdir_events = os.path.join(path, 'events', foldername, 'TTL')\n",
    "            source_path = os.path.join(workdir_events, 'sample_numbers.npy')\n",
    "            destination_path = os.path.join(sync_folder, \"sample_numbers_{}.npy\".format(stream_name))\n",
    "            shutil.copy(source_path, destination_path)\n",
    "        \n",
    "        # run barcode synchronization:\n",
    "        command = ['python', 'barcode_sync_full.py', sync_folder]\n",
    "        result = subprocess.run(command, capture_output=True, text=True)\n",
    "        print(\"Output:\", result.stdout)\n",
    "        if result.stderr:\n",
    "            print(\"Error:\", result.stderr)\n",
    "        \n",
    "\n",
    "    # collect spikedata streams info:\n",
    "    spikedata = {\n",
    "        \"SpikeData\": []\n",
    "    }\n",
    "    for continuous_stream_info in continuous_data_info:\n",
    "        foldername = continuous_stream_info['folder_name']\n",
    "        stream_name = continuous_stream_info['stream_name']\n",
    "\n",
    "        ks_folder = Path(os.path.join(path, \"SpikeData_{}\".format(stream_name)))\n",
    "        new_spikedata = {\n",
    "            \"stream_name\": stream_name,\n",
    "            \"path\": ks_folder.parts\n",
    "        }\n",
    "        spikedata[\"SpikeData\"].append(new_spikedata)\n",
    "    \n",
    "    p = Path(path)\n",
    "    # update and write metadata file:\n",
    "    if run_barcodeSync:\n",
    "        new_recording = {\n",
    "            \"recording_path\": p.parts,\n",
    "            \"SynchData_path\": sync_folder.parts,\n",
    "            \"barcodeSync_output\": result.stdout,\n",
    "            \"barcodeSync_error\": result.stderr,\n",
    "            \"SpikeData\": spikedata[\"SpikeData\"]\n",
    "        }\n",
    "    else:\n",
    "        new_recording = {\n",
    "            \"recording_path\": p.parts,\n",
    "            \"SynchData_path\": sync_folder.parts,\n",
    "            \"SpikeData\": spikedata[\"SpikeData\"]\n",
    "        }\n",
    "    log_metadata[\"recording\"].append(new_recording)\n",
    "    with open(logfilename, 'w') as file:\n",
    "        json.dump(log_metadata, file, indent=4)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if run_preprocessing:\n",
    "        # go on with preprocessing\n",
    "        for stream_name in ap_stream_names[1:]:\n",
    "            # stream_name = continuous_data[index]['stream_name']\n",
    "            # print(index, stream_name)\n",
    "\n",
    "            ks_folder = Path(path) / \"SpikeData_{}\".format(stream_name)\n",
    "            if not ks_folder.is_dir():\n",
    "                ks_folder.mkdir()\n",
    "            \n",
    "            print(\"new dir: \", ks_folder)\n",
    "            recording_raw = se.read_openephys(folder_path=path, stream_name=stream_name, load_sync_channel=False) # this maps all streams even if you specify only one\n",
    "            print(\"Stream name: \", recording_raw.stream_name)\n",
    "            print(recording_raw)\n",
    "\n",
    "            fs = recording_raw.get_sampling_frequency()\n",
    "\n",
    "            noise_levels_microV = si.get_noise_levels(recording_raw, return_scaled=True)\n",
    "            fig, ax = plt.subplots()\n",
    "            _ = ax.hist(noise_levels_microV, bins=np.arange(5, 30, 2.5))\n",
    "            ax.set_xlabel('noise  [microV]')\n",
    "            fig.savefig(os.path.join(ks_folder, 'preprocFigs_noiseLevels.png'))\n",
    "\n",
    "            # preprocessing steps\n",
    "            recording = st.correct_lsb(recording_raw, verbose=1)\n",
    "            recording = st.bandpass_filter(recording, freq_min=300, freq_max=6000)\n",
    "            recording = st.phase_shift(recording) #lazy\n",
    "            [bad_channel_ids, channel_labels] = st.detect_bad_channels(recording=recording)  \n",
    "            recording = recording.remove_channels(remove_channel_ids=bad_channel_ids)  # could be interpolated instead, but why?\n",
    "            # split in groups and apply spatial filtering, then reaggregate. KS4 can now handle multiple shanks\n",
    "            grouped_recordings = recording.split_by(property='group')\n",
    "            recgrouplist_hpsf = [st.highpass_spatial_filter(recording=grouped_recordings[k]) for k in grouped_recordings.keys()]  # cmr is slightly faster. results are similar\n",
    "            recording_hpsf = si.aggregate_channels(recgrouplist_hpsf)\n",
    "            print(recording_hpsf)\n",
    "\n",
    "            # save probe information (as dataframe, plus figure)\n",
    "            probe = recording_hpsf.get_probe() \n",
    "\n",
    "            plot_probe(probe) # for me to see (just which shanks I am dealing with)\n",
    "            plt.gcf().savefig(os.path.join(ks_folder, 'preprocFigs_probe_goodChannels.png'))\n",
    "            df = probe.to_dataframe(complete=True) #complete=True includes channel number\n",
    "            probetablename = Path(os.path.join(ks_folder, f\"probe_table_{stream_name}.csv\"))\n",
    "            df.to_csv(probetablename, index=False)\n",
    "\n",
    "            if callKSfromSI: #takes forever even with latest spikeinterface version. stopped it.\n",
    "                # call kilosort from here, without saving the .bin (will have to call something else to run phy)\n",
    "                print(\"Starting KS4\")\n",
    "                t_start = time.perf_counter()\n",
    "                sorting_ks4_prop = ss.run_sorter_by_property(\"kilosort4\", recording=recording_hpsf, grouping_property=\"group\", working_folder=ks_folder, verbose=True)\n",
    "                t_stop = time.perf_counter()\n",
    "                elapsed_prop = np.round(t_stop - t_start, 2)\n",
    "                print(f\"Elapsed time by property: {elapsed_prop} s\")\n",
    "            else:\n",
    "                # kilosort export .bin continuous data (this can be read by phy too)\n",
    "                # haven't tried the wrapper, pessimistic about the time it takes\n",
    "                probename = \"probe_{}.prb\".format(stream_name)\n",
    "                filename, N, c, s, fs, probe_path = io.spikeinterface_to_binary(\n",
    "                    recording_hpsf, ks_folder, data_name='data.bin', dtype=dtype,\n",
    "                    chunksize=60000, export_probe=True, probe_name=probename\n",
    "                    )\n",
    "                #run KS programmatically\n",
    "                settings = {'fs': fs, 'n_chan_bin': probe.get_contact_count()}\n",
    "                probe_dict = io.load_probe(probe_path)\n",
    "                ops, st_ks, clu, tF, Wall, similar_templates, is_ref, est_contam_rate, kept_spikes = run_kilosort(\n",
    "                                    settings=settings, probe=probe_dict, filename=filename, data_dtype=dtype, device=torch.device(\"cuda\") \n",
    "                )\n",
    "                # decide whether you want to delete data.bin afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a6ff77-3c6d-46a6-938d-3c5b60fa56b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "ts = np.load(r'G:\\Paola Storage do not delete\\RL_E1\\E1_M16\\NPXData\\2024-04-12_12-24-02\\Record Node 101\\experiment1\\recording1\\SynchData\\timestamps_ProbeB-LFP_alignedTo_ProbeB-AP.npy')\n",
    "print(ts)          \n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66f5b69-4b5b-4c3d-8e68-4e561d4d9266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if save2path.is_dir():\n",
    "#     recording_saved = si.read_zarr(save2path)    # reczarr = si.load_extractor(save2path) #achieve the same...\n",
    "# else:\n",
    "#     #recording_saved = recording_hpsf.save(save2path, **job_kwargs)\n",
    "#     import numcodecs\n",
    "#     compressor = numcodecs.Blosc(cname=\"zstd\", clevel=5, shuffle=numcodecs.Blosc.BITSHUFFLE)\n",
    "#     recording_saved = recording_hpsf.save(format=\"zarr\", folder=save2path,\n",
    "#                                          compressor=compressor,\n",
    "#                                          **job_kwargs)\n",
    "# elapsed_time = time.time() - start_time\n",
    "# print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "# print(\"CR custom:\", recording_saved.get_annotation(\"compression_ratio\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511110a0-c89d-48de-93b7-5c1d7525b4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise_levels_unscaled = si.get_noise_levels(recording_saved, return_scaled=False)\n",
    "# fig, ax = plt.subplots()\n",
    "# _ = ax.hist(noise_levels_unscaled, bins=np.arange(5, 30, 2.5))\n",
    "# ax.set_xlabel('noise')\n",
    "\n",
    "# from spikeinterface.sortingcomponents.peak_detection import detect_peaks\n",
    "# from spikeinterface.sortingcomponents.peak_localization import localize_peaks\n",
    "\n",
    "# peaks = detect_peaks(recording_saved,  method='locally_exclusive', noise_levels=noise_levels_unscaled,\n",
    "#                      detect_threshold=5, radius_um=50., **job_kwargs)\n",
    "# peak_locations = localize_peaks(recording_saved, peaks, method='center_of_mass', radius_um=50., **job_kwargs)\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(10, 8))\n",
    "# ax.scatter(peaks['sample_index'] / fs, peak_locations['y'], color='k', marker='.',  alpha=0.02)\n",
    "\n",
    "# elapsed_time = time.time() - start_time\n",
    "# print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "# print(\"CR custom:\", recording_saved.get_annotation(\"compression_ratio\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7de074-c1b6-4d62-ba6c-948b4c95fea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # optional module here:\n",
    "# motion_folder = Path(os.path.join(npxpath, 'motioncorrection'))\n",
    "# rec_corrected, motion_info = st.correct_motion(recording=recording_saved, preset=\"nonrigid_accurate\", folder=motion_folder, output_motion_info=True, **job_kwargs)\n",
    "# # motion_info = st.load_motion_info(motion_folder)\n",
    "# # and plot\n",
    "# fig = plt.figure(figsize=(14, 8))\n",
    "# si.plot_motion(motion_info, figure=fig, depth_lim=(400, 600),\n",
    "#                color_amplitude=True, amplitude_cmap='inferno', scatter_decimate=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ephys-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
