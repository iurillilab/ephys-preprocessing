{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b2d6723-1263-4527-b426-5f633abff40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT HERE ALL FOLDERS TO BE PREPROCESSED:\n",
    "# all recordings and probes (except for LFP) within the following paths will be pre-processed.\n",
    "# specify the path to a general folder containing multiple experiments, or be more specific if you need to exclude something:\n",
    "\n",
    "base_npxpaths = [r\"D:\\M31-npx\"]\n",
    "                # r\"G:\\Paola Storage do not delete\\RL_E1\\E1_M18\",\n",
    "                # r\"G:\\Paola Storage do not delete\\RL_E1\\E1_M16\",\n",
    "run_barcodeSync = False\n",
    "run_preprocessing = True # run preprocessing and spikesorting\n",
    "callKSfromSI = False  # this remains insanely slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95e0c296-d890-4db7-ae68-b27e6cd1e780",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nwb-conv not installed!\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import scipy.signal\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "import spikeinterface.extractors as se\n",
    "import spikeinterface.full as si\n",
    "import spikeinterface.preprocessing as st\n",
    "import spikeinterface.sorters as ss\n",
    "from preprocessing_utils import make_probe_plots, standard_preprocessing\n",
    "\n",
    "from probeinterface import ProbeGroup, write_prb\n",
    "from probeinterface.plotting import plot_probe, plot_probe_group\n",
    "\n",
    "from kilosort import io, run_kilosort\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "# %matplotlib  widget\n",
    "\n",
    "n_cpus = os.cpu_count()\n",
    "n_jobs = n_cpus - 4\n",
    "print(n_jobs)\n",
    "job_kwargs = dict(n_jobs=n_jobs, chunk_duration=\"1s\", progress_bar=True)\n",
    "\n",
    "dtype = np.int16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3bc76e1-9053-4d09-99d7-a2c6483a6f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEXT_FEATURES (tentative):\n",
    "# add programmatic call to KS (not via SI - or perhaps not without saving preprocessed data to disk.. otherwise takes impossibly long)\n",
    "\n",
    "# # add an option to either save data.bin and ks output to a different drive/location if a faster drive is available, and then transfer it all to the original destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc938dda-7b30-44eb-b099-d1c206554a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will be processing all probes from the following recordings:\n",
      "D:\\M31-npx\\2025-01-31_14-59-30\\Record Node 107\\experiment1\\recording1\n",
      "ProbeA\n",
      "ProbeB-AP\n",
      "[0, 1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "found_paths = [] #List to store all found paths\n",
    "target_file = 'structure.oebin'\n",
    "\n",
    "def search_files(base_path, target_file):\n",
    "    for dirpath, dirnames, filenames in os.walk(base_path):\n",
    "        if target_file in filenames:\n",
    "            found_paths.append(dirpath)\n",
    "for path in base_npxpaths:\n",
    "    search_files(path, target_file)\n",
    "\n",
    "if found_paths:\n",
    "    print(\"Will be processing all probes from the following recordings:\")\n",
    "    for path in found_paths:\n",
    "        print(path)\n",
    "        full_path = os.path.join(path, target_file)\n",
    "        with open(full_path, 'r') as file:\n",
    "            file_contents = file.read()\n",
    "            data = json.loads(file_contents)\n",
    "            continuous_data = data['continuous']\n",
    "            indices_notLFP = [index for index, folder in enumerate(continuous_data)\n",
    "                                   if \"LFP\" not in folder['folder_name'] and \"NI-DAQ\" not in folder['folder_name']]\n",
    "            for index in indices_notLFP:\n",
    "                stream_name = continuous_data[index]['stream_name']\n",
    "                print(stream_name)\n",
    "else:\n",
    "    print(\"File 'structure.oebin' not found in any of the specified directories\")\n",
    "\n",
    "    \n",
    "# log relevant preprocessing metadata\n",
    "now = datetime.now()\n",
    "formatted_time = now.strftime('%Y%m%d_%H%M%S')\n",
    "logfilename = f\"log_preprocessing_{formatted_time}.json\" \n",
    "# initialize a dictionary that will be updated dynamically\n",
    "log_metadata = {\n",
    "    \"recording\": []\n",
    "}\n",
    "print(indices_notLFP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9de0d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find json file with recording structure\n",
    "from pprint import pprint\n",
    "base_path = base_npxpaths[0]\n",
    "\n",
    "def find_recinfo_file(base_path, target_file='structure.oebin'):\n",
    "    file_path = list(Path(base_path).rglob(target_file))\n",
    "    assert len(file_path) == 1, f\"Found {len(file_path)} files with name {target_file} in {base_path}. Please specify a single file.\"\n",
    "    return file_path[0]\n",
    "\n",
    "def read_continuous_data_info(recinfo_file):\n",
    "    with open(recinfo_file, 'r') as f:\n",
    "        file_contents = f.read()\n",
    "        rec_info = json.loads(file_contents)\n",
    "    # pprint(rec_info)\n",
    "    return rec_info[\"continuous\"]\n",
    "\n",
    "def get_channel_names(continuous_data_info):\n",
    "    string_to_exclude = ['LFP', 'NI-DAQ']\n",
    "    # recinfo = load_recinfo(base_path)\n",
    "    all_folders = []\n",
    "    ap_folders = []\n",
    "    for folder in continuous_data_info:\n",
    "        stream_name = folder[\"recorded_processor\"] + \" \" + str(folder[\"recorded_processor_id\"]) + \"#\" + folder['folder_name'][:-1]\n",
    "        all_folders.append(stream_name)\n",
    "\n",
    "        if all([string not in folder['folder_name'] for string in string_to_exclude]):\n",
    "            ap_folders.append(stream_name)\n",
    "    \n",
    "    return all_folders, ap_folders\n",
    "\n",
    "recinfo_file = find_recinfo_file(base_path)\n",
    "continuous_data_info = read_continuous_data_info(recinfo_file)\n",
    "all_stream_names, ap_stream_names = get_channel_names(continuous_data_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3722e981-c5b3-45e2-9989-c32962c01b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\M31-npx\\2025-01-31_14-59-30\\Record Node 107\\experiment1\\recording1\\SynchData\n"
     ]
    }
   ],
   "source": [
    "# if sync data were collected through openephys, you can proceed directly (if you read this, it might need debugging)\n",
    "\n",
    "# if sync data were collected through intan (legacy), pause here and extract the barcode files first (done in matlab)\n",
    "# then move the SynchData folder to the corresponding paths shown above\n",
    "for path in found_paths:\n",
    "    sync_folder = Path(os.path.join(path, 'SynchData')) # please do not change the spelling\n",
    "    sync_folder.mkdir(exist_ok=True)\n",
    "    print(sync_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a8c15d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new dir:  D:\\M31-npx\\2025-01-31_14-59-30\\Record Node 107\\experiment1\\recording1\\SpikeData_Record Node 107#Neuropix-PXI-100.ProbeA\n",
      "Stream name:  Record Node 107#Neuropix-PXI-100.ProbeA\n",
      "OpenEphysBinaryRecordingExtractor: 384 channels - 30.0kHz - 1 segments - 193,516,844 samples \n",
      "                                   6,450.56s (1.79 hours) - int16 dtype - 138.41 GiB\n",
      "Estimated LSB value: 15\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "make_probe_plots() missing 1 required positional argument: 'stream_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 98\u001b[0m\n\u001b[0;32m     95\u001b[0m recording_hpsf \u001b[38;5;241m=\u001b[39m standard_preprocessing(recording_raw)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# save probe information (as dataframe, plus figure)\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m \u001b[43mmake_probe_plots\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecording_raw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecording_hpsf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mks_folder\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: make_probe_plots() missing 1 required positional argument: 'stream_name'"
     ]
    }
   ],
   "source": [
    "run_barcodeSync = False\n",
    "run_preprocessing = True # run preprocessing and spikesorting\n",
    "callKSfromSI = False  # this remains insanely slow.\n",
    "\n",
    "for base_path in base_npxpaths:\n",
    "    recinfo_file = find_recinfo_file(base_path)\n",
    "    continuous_data_info = read_continuous_data_info(recinfo_file)\n",
    "    all_stream_names, ap_stream_names = get_channel_names(continuous_data_info)\n",
    "\n",
    "    # extra step to correct for bad timestamps from failed online synchronization here:\n",
    "    sync_folder = Path(path) / 'SynchData'  # please do not change the spelling\n",
    "    \n",
    "    if run_barcodeSync:\n",
    "        sync_folder.mkdir(exist_ok=True)\n",
    "        \n",
    "        for continuous_stream_info in continuous_data_info:\n",
    "            foldername = continuous_stream_info['folder_name']\n",
    "            stream_name = continuous_stream_info['stream_name']\n",
    "            sample_rate = continuous_stream_info['sample_rate']\n",
    "            workdir = os.path.join(path, 'continuous', foldername)\n",
    "            sn = np.load(os.path.join(workdir, 'sample_numbers.npy'))\n",
    "            ts = sn.astype(np.float64)\n",
    "            ts = ts/sample_rate\n",
    "            ts = ts.reshape(-1,1) #make it consistent with what is required downstream, and with what is saved by matlab\n",
    "            print(ts.shape)\n",
    "            np.save(os.path.join(sync_folder, \"timestamps_{}.npy\".format(stream_name)), ts)\n",
    "            # also retrieve all the barcode files and copy them to the same sync folder\n",
    "            workdir_events = os.path.join(path, 'events', foldername, 'TTL')\n",
    "            source_path = os.path.join(workdir_events, 'sample_numbers.npy')\n",
    "            destination_path = os.path.join(sync_folder, \"sample_numbers_{}.npy\".format(stream_name))\n",
    "            shutil.copy(source_path, destination_path)\n",
    "        \n",
    "        # run barcode synchronization:\n",
    "        command = ['python', 'barcode_sync_full.py', sync_folder]\n",
    "        result = subprocess.run(command, capture_output=True, text=True)\n",
    "        print(\"Output:\", result.stdout)\n",
    "        if result.stderr:\n",
    "            print(\"Error:\", result.stderr)\n",
    "        \n",
    "\n",
    "    # collect spikedata streams info:\n",
    "    spikedata = {\n",
    "        \"SpikeData\": []\n",
    "    }\n",
    "    for continuous_stream_info in continuous_data_info:\n",
    "        foldername = continuous_stream_info['folder_name']\n",
    "        stream_name = continuous_stream_info['stream_name']\n",
    "\n",
    "        ks_folder = Path(os.path.join(path, \"SpikeData_{}\".format(stream_name)))\n",
    "        new_spikedata = {\n",
    "            \"stream_name\": stream_name,\n",
    "            \"path\": ks_folder.parts\n",
    "        }\n",
    "        spikedata[\"SpikeData\"].append(new_spikedata)\n",
    "    \n",
    "    p = Path(path)\n",
    "    # update and write metadata file:\n",
    "    if run_barcodeSync:\n",
    "        new_recording = {\n",
    "            \"recording_path\": p.parts,\n",
    "            \"SynchData_path\": sync_folder.parts,\n",
    "            \"barcodeSync_output\": result.stdout,\n",
    "            \"barcodeSync_error\": result.stderr,\n",
    "            \"SpikeData\": spikedata[\"SpikeData\"]\n",
    "        }\n",
    "    else:\n",
    "        new_recording = {\n",
    "            \"recording_path\": p.parts,\n",
    "            \"SynchData_path\": sync_folder.parts,\n",
    "            \"SpikeData\": spikedata[\"SpikeData\"]\n",
    "        }\n",
    "    log_metadata[\"recording\"].append(new_recording)\n",
    "    with open(logfilename, 'w') as file:\n",
    "        json.dump(log_metadata, file, indent=4)\n",
    "    \n",
    "    \n",
    "    if run_preprocessing:\n",
    "        # go on with preprocessing\n",
    "        for stream_name in ap_stream_names:\n",
    "            # stream_name = continuous_data[index]['stream_name']\n",
    "            # print(index, stream_name)\n",
    "\n",
    "            ks_folder = Path(path) / \"SpikeData_{}\".format(stream_name)\n",
    "            ks_folder.mkdir(exist_ok=True)\n",
    "            \n",
    "            print(\"new dir: \", ks_folder)\n",
    "            recording_raw = se.read_openephys(folder_path=path, stream_name=stream_name, load_sync_channel=False) # this maps all streams even if you specify only one\n",
    "            print(\"Stream name: \", recording_raw.stream_name)\n",
    "            print(recording_raw)\n",
    "\n",
    "            fs = recording_raw.get_sampling_frequency()\n",
    "\n",
    "\n",
    "            # preprocessing steps\n",
    "            recording_hpsf = standard_preprocessing(recording_raw)\n",
    "\n",
    "            # save probe information (as dataframe, plus figure)\n",
    "            make_probe_plots(recording_raw, recording_hpsf, ks_folder, stream_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee2ab489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recording_raw.get_probes()[0].get_shank_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a6ff77-3c6d-46a6-938d-3c5b60fa56b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "ts = np.load(r'G:\\Paola Storage do not delete\\RL_E1\\E1_M16\\NPXData\\2024-04-12_12-24-02\\Record Node 101\\experiment1\\recording1\\SynchData\\timestamps_ProbeB-LFP_alignedTo_ProbeB-AP.npy')\n",
    "print(ts)          \n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66f5b69-4b5b-4c3d-8e68-4e561d4d9266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if save2path.is_dir():\n",
    "#     recording_saved = si.read_zarr(save2path)    # reczarr = si.load_extractor(save2path) #achieve the same...\n",
    "# else:\n",
    "#     #recording_saved = recording_hpsf.save(save2path, **job_kwargs)\n",
    "#     import numcodecs\n",
    "#     compressor = numcodecs.Blosc(cname=\"zstd\", clevel=5, shuffle=numcodecs.Blosc.BITSHUFFLE)\n",
    "#     recording_saved = recording_hpsf.save(format=\"zarr\", folder=save2path,\n",
    "#                                          compressor=compressor,\n",
    "#                                          **job_kwargs)\n",
    "# elapsed_time = time.time() - start_time\n",
    "# print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "# print(\"CR custom:\", recording_saved.get_annotation(\"compression_ratio\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511110a0-c89d-48de-93b7-5c1d7525b4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise_levels_unscaled = si.get_noise_levels(recording_saved, return_scaled=False)\n",
    "# fig, ax = plt.subplots()\n",
    "# _ = ax.hist(noise_levels_unscaled, bins=np.arange(5, 30, 2.5))\n",
    "# ax.set_xlabel('noise')\n",
    "\n",
    "# from spikeinterface.sortingcomponents.peak_detection import detect_peaks\n",
    "# from spikeinterface.sortingcomponents.peak_localization import localize_peaks\n",
    "\n",
    "# peaks = detect_peaks(recording_saved,  method='locally_exclusive', noise_levels=noise_levels_unscaled,\n",
    "#                      detect_threshold=5, radius_um=50., **job_kwargs)\n",
    "# peak_locations = localize_peaks(recording_saved, peaks, method='center_of_mass', radius_um=50., **job_kwargs)\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(10, 8))\n",
    "# ax.scatter(peaks['sample_index'] / fs, peak_locations['y'], color='k', marker='.',  alpha=0.02)\n",
    "\n",
    "# elapsed_time = time.time() - start_time\n",
    "# print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "# print(\"CR custom:\", recording_saved.get_annotation(\"compression_ratio\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7de074-c1b6-4d62-ba6c-948b4c95fea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # optional module here:\n",
    "# motion_folder = Path(os.path.join(npxpath, 'motioncorrection'))\n",
    "# rec_corrected, motion_info = st.correct_motion(recording=recording_saved, preset=\"nonrigid_accurate\", folder=motion_folder, output_motion_info=True, **job_kwargs)\n",
    "# # motion_info = st.load_motion_info(motion_folder)\n",
    "# # and plot\n",
    "# fig = plt.figure(figsize=(14, 8))\n",
    "# si.plot_motion(motion_info, figure=fig, depth_lim=(400, 600),\n",
    "#                color_amplitude=True, amplitude_cmap='inferno', scatter_decimate=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ephys-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
